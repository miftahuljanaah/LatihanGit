{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>lang</th>\n",
       "      <th>user_id_str</th>\n",
       "      <th>conversation_id_str</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tue Aug 29 23:59:24 +0000 2023</td>\n",
       "      <td>1,69667E+18</td>\n",
       "      <td>-ness niat baik ngasi job ke anak Unnes, malah...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>in</td>\n",
       "      <td>1,18728E+18</td>\n",
       "      <td>1,69667E+18</td>\n",
       "      <td>unnesmenfess</td>\n",
       "      <td>https://twitter.com/unnesmenfess/status/169667...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tue Aug 29 23:47:23 +0000 2023</td>\n",
       "      <td>1,69667E+18</td>\n",
       "      <td>@unnesmenfess https://t.co/PiKDLydxPP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>qme</td>\n",
       "      <td>1,67212E+18</td>\n",
       "      <td>1,69667E+18</td>\n",
       "      <td>jekbersinergi</td>\n",
       "      <td>https://twitter.com/jekbersinergi/status/16966...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tue Aug 29 23:44:16 +0000 2023</td>\n",
       "      <td>1,69667E+18</td>\n",
       "      <td>-ness need anjem jam 7.35 area unnes</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>1,18728E+18</td>\n",
       "      <td>1,69667E+18</td>\n",
       "      <td>unnesmenfess</td>\n",
       "      <td>https://twitter.com/unnesmenfess/status/169667...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tue Aug 29 23:41:34 +0000 2023</td>\n",
       "      <td>1,69667E+18</td>\n",
       "      <td>-ness ada anjem cewe yang bisa nganter jam 7 p...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>in</td>\n",
       "      <td>1,18728E+18</td>\n",
       "      <td>1,69667E+18</td>\n",
       "      <td>unnesmenfess</td>\n",
       "      <td>https://twitter.com/unnesmenfess/status/169666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tue Aug 29 23:32:53 +0000 2023</td>\n",
       "      <td>1,69667E+18</td>\n",
       "      <td>@FitrahDharul @alqodrifaqih @aniesbaswedan @BE...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>in</td>\n",
       "      <td>1,45251E+18</td>\n",
       "      <td>1,69644E+18</td>\n",
       "      <td>EviDrajat</td>\n",
       "      <td>https://twitter.com/EviDrajat/status/169666726...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at       id_str  \\\n",
       "0  Tue Aug 29 23:59:24 +0000 2023  1,69667E+18   \n",
       "1  Tue Aug 29 23:47:23 +0000 2023  1,69667E+18   \n",
       "2  Tue Aug 29 23:44:16 +0000 2023  1,69667E+18   \n",
       "3  Tue Aug 29 23:41:34 +0000 2023  1,69667E+18   \n",
       "4  Tue Aug 29 23:32:53 +0000 2023  1,69667E+18   \n",
       "\n",
       "                                                text  quote_count  \\\n",
       "0  -ness niat baik ngasi job ke anak Unnes, malah...            0   \n",
       "1              @unnesmenfess https://t.co/PiKDLydxPP            0   \n",
       "2               -ness need anjem jam 7.35 area unnes            0   \n",
       "3  -ness ada anjem cewe yang bisa nganter jam 7 p...            0   \n",
       "4  @FitrahDharul @alqodrifaqih @aniesbaswedan @BE...            0   \n",
       "\n",
       "   reply_count  retweet_count  favorite_count lang  user_id_str  \\\n",
       "0            2              0               2   in  1,18728E+18   \n",
       "1            0              0               0  qme  1,67212E+18   \n",
       "2            7              0               0   en  1,18728E+18   \n",
       "3            6              0               0   in  1,18728E+18   \n",
       "4            1              0               1   in  1,45251E+18   \n",
       "\n",
       "  conversation_id_str       username  \\\n",
       "0         1,69667E+18   unnesmenfess   \n",
       "1         1,69667E+18  jekbersinergi   \n",
       "2         1,69667E+18   unnesmenfess   \n",
       "3         1,69667E+18   unnesmenfess   \n",
       "4         1,69644E+18      EviDrajat   \n",
       "\n",
       "                                           tweet_url  \n",
       "0  https://twitter.com/unnesmenfess/status/169667...  \n",
       "1  https://twitter.com/jekbersinergi/status/16966...  \n",
       "2  https://twitter.com/unnesmenfess/status/169667...  \n",
       "3  https://twitter.com/unnesmenfess/status/169666...  \n",
       "4  https://twitter.com/EviDrajat/status/169666726...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('D:/KULIAH/PRIGEL/KODING/ANALISIS/DATASET/TWITTER/unnes29082023.csv', delimiter=';')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values: 0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\KULIAH\\PRIGEL\\KODING\\ANALISIS\\analisis_twitter.ipynb Cell 3\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/KULIAH/PRIGEL/KODING/ANALISIS/analisis_twitter.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m TAG_CLEANING_RE \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m@\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mS+\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/KULIAH/PRIGEL/KODING/ANALISIS/analisis_twitter.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Remove @tags\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/KULIAH/PRIGEL/KODING/ANALISIS/analisis_twitter.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m X[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m X[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x: re\u001b[39m.\u001b[39msub(TAG_CLEANING_RE, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KULIAH/PRIGEL/KODING/ANALISIS/analisis_twitter.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Smart lowercase\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KULIAH/PRIGEL/KODING/ANALISIS/analisis_twitter.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m X[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m X[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mlower())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3806\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "# Text-preprocessing\n",
    "\n",
    "# Missing Values\n",
    "num_missing_desc = data.isnull().sum()[2]    # No. of values with msising descriptions\n",
    "print('Number of missing values: ' + str(num_missing_desc))\n",
    "data = data.dropna()\n",
    "\n",
    "TAG_CLEANING_RE = \"@\\S+\"\n",
    "# Remove @tags\n",
    "X['text'] = X['text'].map(lambda x: re.sub(TAG_CLEANING_RE, ' ', x))\n",
    "\n",
    "# Smart lowercase\n",
    "X['text'] = X['text'].map(lambda x: x.lower())\n",
    "\n",
    "# Remove numbers\n",
    "X['text'] = X['text'].map(lambda x: re.sub(r'\\d+', ' ', x))\n",
    "\n",
    "# Remove links\n",
    "TEXT_CLEANING_RE = \"https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "X['text'] = X['text'].map(lambda x: re.sub(TEXT_CLEANING_RE, ' ', x))\n",
    "\n",
    "# Remove Punctuation\n",
    "X['text']  = X['text'].map(lambda x: x.translate(x.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# Remove white spaces\n",
    "X['text'] = X['text'].map(lambda x: x.strip())\n",
    "\n",
    "# Tokenize into words\n",
    "X['text'] = X['text'].map(lambda x: word_tokenize(x))\n",
    " \n",
    "# Remove non alphabetic tokens\n",
    "X['text'] = X['text'].map(lambda x: [word for word in x if word.isalpha()])\n",
    "\n",
    "# Filter out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "X['text'] = X['text'].map(lambda x: [w for w in x if not w in stop_words])\n",
    "    \n",
    "# Word Lemmatization\n",
    "lem = WordNetLemmatizer()\n",
    "X['text'] = X['text'].map(lambda x: [lem.lemmatize(word,\"v\") for word in x])\n",
    "\n",
    "# Turn lists back to string\n",
    "X['text'] = X['text'].map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['created_at', 'id_str', 'text', 'quote_count', 'reply_count',\n",
      "       'retweet_count', 'favorite_count', 'lang', 'user_id_str',\n",
      "       'conversation_id_str', 'username', 'tweet_url'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_COLUMNS = ['created_at', 'id_str', 'text', 'quote_count', 'reply_count',\n",
    "       'retweet_count', 'favorite_count', 'lang', 'user_id_str',\n",
    "       'conversation_id_str', 'username', 'tweet_url']\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "data = pd.read_csv('D:/KULIAH/PRIGEL/KODING/ANALISIS/DATASET/TWITTER/unnes29082023.csv', delimiter=';', encoding =DATASET_ENCODING , names=DATASET_COLUMNS)\n",
    "data.head()\n",
    "X = data.iloc[:,[5]]\n",
    "Y = data.iloc[:,0]\n",
    "Y[Y == 4] = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
